<html>
<head>
<META http-equiv="Content-Type" content="text/html; charset=EUC-JP">
<title>Module mpp_io_mod</title>
<link type="text/css" href="http://www.gfdl.noaa.gov/~fms/style/doc.css" rel="stylesheet">
<STYLE TYPE="text/css">
          .fixed {
            font-size:medium;
            font-family:monospace;
            border-style:none;
            border-width:0.1em;
            padding:0.1em;
            color:#663366;
          }
        </STYLE>
</head>
<body>
<a name="TOP"></a><font class="header" size="1"><a href="#PUBLIC%20INTERFACE">PUBLIC INTERFACE </a>~
          <a href="#PUBLIC%20DATA">PUBLIC DATA </a>~
          <a href="#PUBLIC%20ROUTINES">PUBLIC ROUTINES </a>~
          <a href="#NAMELIST">NAMELIST </a>~
          <a href="#DIAGNOSTIC%20FIELDS">DIAGNOSTIC FIELDS </a>~
          <a href="#ERROR%20MESSAGES">ERROR MESSAGES </a>~
          <a href="#REFERENCES">REFERENCES </a>~ 
          <a href="#NOTES">NOTES</a></font>
<hr>
<h2>Module mpp_io_mod</h2>
<a name="HEADER"></a>
<!-- BEGIN HEADER -->
<div>
<b>Contact:&nbsp;</b><a href="mailto:vb@gfdl.noaa.gov">   V. Balaji </a>
<br>
<b>Reviewers:&nbsp;</b>
<br>
<b>Change History:&nbsp;</b><a href="http://www.gfdl.noaa.gov/fms-cgi-bin/cvsweb.cgi/FMS/shared/mpp">WebCVS Log</a>
<br>
<b>RCS Log:&nbsp;</b><a href="http://www.gfdl.noaa.gov/~vb/changes_mpp_io.html">RCS Log</a>
<br>
<br>
</div>
<!-- END HEADER -->
<a name="OVERVIEW"></a>
<hr>
<h4>OVERVIEW</h4>
<!-- BEGIN OVERVIEW -->
<p class="text"> 
<tt>mpp_io_mod</tt>, is a set of simple calls for parallel I/O on
   distributed systems. It is geared toward the writing of data in netCDF
   format. It requires the modules <a href="mpp_domains.html">mpp_domains_mod</a>   and <a href="mpp.html">mpp_mod</a>, upon which it is built. </p>
<!-- END OVERVIEW -->
<a name="DESCRIPTION"></a>
<!-- BEGIN DESCRIPTION -->
<div>   In massively parallel environments, an often difficult problem is
   the reading and writing of data to files on disk. MPI-IO and MPI-2 IO
   are moving toward providing this capability, but are currently not
   widely implemented. Further, it is a rather abstruse
   API. <tt>mpp_io_mod</tt>   is an attempt at a simple API encompassing a
   certain variety of the I/O tasks that will be required. It does not
   attempt to be an all-encompassing standard such as MPI, however, it
   can be implemented in MPI if so desired. It is equally simple to add
   parallel I/O capability to <tt>mpp_io_mod</tt>   based on vendor-specific
   APIs while providing a layer of insulation for user codes.
   <br>
<br>
   The <tt>mpp_io_mod</tt>   parallel I/O API built on top of the <a href="mpp_domains.html">mpp_domains_mod</a>   and <a href="mpp.html">mpp_mod</a>   API for domain decomposition and
   message passing. Features of <tt>mpp_io_mod</tt>   include:
   <br>
<br>
   1) Simple, minimal API, with free access to underlying API for more
   complicated stuff.<br>   2) Self-describing files: comprehensive header information
   (metadata) in the file itself.<br>   3) Strong focus on performance of parallel write: the climate models
   for which it is designed typically read a minimal amount of data
   (typically at the beginning of the run); but on the other hand, tend
   to write copious amounts of data during the run. An interface for
   reading is also supplied, but its performance has not yet been optimized.<br>   4) Integrated netCDF capability: <a href="http://www.unidata.ucar.edu/packages/netcdf/">netCDF</a>   is a
   data format widely used in the climate/weather modeling
   community. netCDF is considered the principal medium of data storage
   for <tt>mpp_io_mod</tt>. But I provide a raw unformatted
   fortran I/O capability in case netCDF is not an option, either due to
   unavailability, inappropriateness, or poor performance.<br>   5) May require off-line post-processing: a tool for this purpose, <tt>mppnccombine</tt>, is available. GFDL users may use <tt>~hnv/pub/mppnccombine</tt>. Outside users may obtain the
   source <a href="ftp://ftp.gfdl.gov/perm/hnv/mpp/mppnccombine.c">here</a>.  It
   can be compiled on any C compiler and linked with the netCDF
   library. The program is free and is covered by the <a href="ftp://ftp.gfdl.gov/perm/hnv/mpp/LICENSE">GPL license</a>.
   <br>
<br>
   The internal representation of the data being written out is
   assumed be the default real type, which can be 4 or 8-byte. Time data
   is always written as 8-bytes to avoid overflow on climatic time scales
   in units of seconds.
   <br>
<br> 
<a href="modes"></a>
<h4>I/O modes in <tt>mpp_io_mod</tt>
</h4>   The I/O activity critical to performance in the models for which <tt>mpp_io_mod</tt>   is designed is typically the writing of large
   datasets on a model grid volume produced at intervals during
   a run. Consider a 3D grid volume, where model arrays are stored as <tt>(i,j,k)</tt>. The domain decomposition is typically along <tt>i</tt>   or <tt>j</tt>: thus to store data to disk as a global
   volume, the distributed chunks of data have to be seen as
   non-contiguous. If we attempt to have all PEs write this data into a
   single file, performance can be seriously compromised because of the
   data reordering that will be required. Possible options are to have
   one PE acquire all the data and write it out, or to have all the PEs
   write independent files, which are recombined offline. These three
   modes of operation are described in the <tt>mpp_io_mod</tt>   terminology
   in terms of two parameters, <i>threading</i>   and <i>fileset</i>,
   as follows:
   <br>
<br> 
<i>Single-threaded I/O:</i>   a single PE acquires all the data
   and writes it out.<br> 
<i>Multi-threaded, single-fileset I/O:</i>   many PEs write to a
   single file.<br> 
<i>Multi-threaded, multi-fileset I/O:</i>   many PEs write to
   independent files. This is also called <i>distributed I/O</i>.
   <br>
<br>
   The middle option is the most difficult to achieve performance. The
   choice of one of these modes is made when a file is opened for I/O, in <a href="#mpp_open">mpp_open</a>.
   <br>
<br> 
<a href=""></a>
<h4>Metadata in <tt>mpp_io_mod</tt>
</h4>   A requirement of the design of <tt>mpp_io_mod</tt>   is that the file must
   be entirely self-describing: comprehensive header information
   describing its contents is present in the header of every file. The
   header information follows the model of netCDF. Variables in the file
   are divided into <i>axes</i>   and <i>fields</i>. An axis describes a
   co-ordinate variable, e.g <tt>x,y,z,t</tt>. A field consists of data in
   the space described by the axes. An axis is described in <tt>mpp_io_mod</tt>   using the defined type <tt>axistype</tt>:
   <br>
<br> 
<pre>   type, public :: axistype
      sequence
      character(len=128) :: name
      character(len=128) :: units
      character(len=256) :: longname
      character(len=8) :: cartesian
      integer :: len
      integer :: sense           !+/-1, depth or height?
      type(domain1D), pointer :: domain
      real, dimension(:), pointer :: data
      integer :: id, did
      integer :: type  ! external NetCDF type format for axis data
      integer :: natt
      type(atttype), pointer :: Att(:) ! axis attributes
   end type axistype</pre>   A field is described using the type <tt>fieldtype</tt>:
   <br>
<br> 
<pre>   type, public :: fieldtype
      sequence
      character(len=128) :: name
      character(len=128) :: units
      character(len=256) :: longname
      real :: min, max, missing, fill, scale, add
      integer :: pack
      type(axistype), dimension(:), pointer :: axes
      integer, dimension(:), pointer :: size
      integer :: time_axis_index
      integer :: id
      integer :: type ! external NetCDF format for field data
      integer :: natt, ndim
      type(atttype), pointer :: Att(:) ! field metadata
   end type fieldtype</pre>   An attribute (global, field or axis) is described using the <tt>atttype</tt>:
   <br>
<br> 
<pre>   type, public :: atttype
      sequence
      integer :: type, len
      character(len=128) :: name
      character(len=256)  :: catt
      real(FLOAT_KIND), pointer :: fatt(:)
   end type atttype</pre> 
<a href=""></a>This default set of field attributes corresponds
   closely to various conventions established for netCDF files. The <tt>pack</tt>   attribute of a field defines whether or not a
   field is to be packed on output. Allowed values of <tt>pack</tt>   are 1,2,4 and 8. The value of <tt>pack</tt>   is the number of variables written into 8
   bytes. In typical use, we write 4-byte reals to netCDF output; thus
   the default value of <tt>pack</tt>   is 2. For <tt>pack</tt>   = 4 or 8, packing uses a simple-minded linear
   scaling scheme using the <tt>scale</tt>   and <tt>add</tt>   attributes. There is thus likely to be a significant loss of dynamic
   range with packing. When a field is declared to be packed, the <tt>missing</tt>   and <tt>fill</tt>   attributes, if
   supplied, are packed also.
   <br>
<br>
   Please note that the pack values are the same even if the default
   real is 4 bytes, i.e <tt>PACK=1</tt>   still follows the definition
   above and writes out 8 bytes.
   <br>
<br>
   A set of <i>attributes</i>   for each variable is also available. The
   variable definitions and attribute information is written/read by calling <a href="#mpp_write_meta">mpp_write_meta</a>   or <a href="#mpp_read_meta">mpp_read_meta</a>. A typical calling
   sequence for writing data might be:
   <br>
<br> 
<pre>   ...
     type(domain2D), dimension(:), allocatable, target :: domain
     type(fieldtype) :: field
     type(axistype) :: x, y, z, t
   ...
     call mpp_define_domains( (/1,nx,1,ny/), domain )
     allocate( a(domain(pe)%x%data%start_index:domain(pe)%x%data%end_index, &amp;
                 domain(pe)%y%data%start_index:domain(pe)%y%data%end_index,nz) )
   ...
     call mpp_write_meta( unit, x, 'X', 'km', 'X distance', &amp;
          domain=domain(pe)%x, data=(/(float(i),i=1,nx)/) )
     call mpp_write_meta( unit, y, 'Y', 'km', 'Y distance', &amp;
          domain=domain(pe)%y, data=(/(float(i),i=1,ny)/) )
     call mpp_write_meta( unit, z, 'Z', 'km', 'Z distance', &amp;
          data=(/(float(i),i=1,nz)/) )
     call mpp_write_meta( unit, t, 'Time', 'second', 'Time' )
   
     call mpp_write_meta( unit, field, (/x,y,z,t/), 'a', '(m/s)', AAA', &amp;
          missing=-1e36 )
   ...
     call mpp_write( unit, x )
     call mpp_write( unit, y )
     call mpp_write( unit, z )
   ...</pre>   In this example, <tt>x</tt>   and <tt>y</tt>   have been
   declared as distributed axes, since a domain decomposition has been
   associated. <tt>z</tt>   and <tt>t</tt>   are undistributed
   axes. <tt>t</tt>   is known to be a <i>record</i>   axis (netCDF
   terminology) since we do not allocate the <tt>data</tt>   element
   of the <tt>axistype</tt>. <i>Only one record axis may be
   associated with a file.</i>   The call to <a href="#mpp_write_meta">mpp_write_meta</a>   initializes
   the axes, and associates a unique variable ID with each axis. The call
   to <tt>mpp_write_meta</tt>   with argument <tt>field</tt>   declared <tt>field</tt>   to be a 4D variable that is a function
   of <tt>(x,y,z,t)</tt>, and a unique variable ID is associated
   with it. A 3D field will be written at each call to <tt>mpp_write(field)</tt>.
   <br>
<br>
   The data to any variable, including axes, is written by <tt>mpp_write</tt>.
   <br>
<br>
   Any additional attributes of variables can be added through
   subsequent <tt>mpp_write_meta</tt>   calls, using the variable ID as a
   handle. <i>Global</i>   attributes, associated with the dataset as a
   whole, can also be written thus. See the <a href="#mpp_write_meta">mpp_write_meta</a>   call syntax below
   for further details.
   <br>
<br>
   You cannot interleave calls to <tt>mpp_write</tt>   and <tt>mpp_write_meta</tt>: the first call to <tt>mpp_write</tt>   implies that metadata specification is
   complete.
   <br>
<br>
   A typical calling sequence for reading data might be:
   <br>
<br> 
<pre>   ...
     integer :: unit, natt, nvar, ntime
     type(domain2D), dimension(:), allocatable, target :: domain
     type(fieldtype), allocatable, dimension(:) :: fields
     type(atttype), allocatable, dimension(:) :: global_atts
     real, allocatable, dimension(:) :: times
   ...
     call mpp_define_domains( (/1,nx,1,ny/), domain )
   
     call mpp_read_meta(unit)
     call mpp_get_info(unit,natt,nvar,ntime)
     allocate(global_atts(natt))
     call mpp_get_atts(unit,global_atts)
     allocate(fields(nvar))
     call mpp_get_vars(unit, fields)
     allocate(times(ntime))
     call mpp_get_times(unit, times)
   
     allocate( a(domain(pe)%x%data%start_index:domain(pe)%x%data%end_index, &amp;
                 domain(pe)%y%data%start_index:domain(pe)%y%data%end_index,nz) )
   ...
     do i=1, nvar
       if (fields(i)%name == 'a')  call mpp_read(unit,fields(i),domain(pe), a,
                                                 tindex)
     enddo
   ...</pre>   In this example, the data are distributed as in the previous
   example. The call to <a href="#mpp_read_meta">mpp_read_meta</a>   initializes
   all of the metadata associated with the file, including global
   attributes, variable attributes and non-record dimension data. The
   call to <tt>mpp_get_info</tt>   returns the number of global
   attributes (<tt>natt</tt>), variables (<tt>nvar</tt>) and
   time levels (<tt>ntime</tt>) associated with the file
   identified by a unique ID (<tt>unit</tt>). <tt>mpp_get_atts</tt>   returns all global attributes for
   the file in the derived type <tt>atttype(natt)</tt>. <tt>mpp_get_vars</tt>   returns variable types
   (<tt>fieldtype(nvar)</tt>).  Since the record dimension data are not allocated for calls to <a href="#mpp_write">mpp_write</a>, a separate call to <tt>mpp_get_times</tt>   is required to access record dimension data.  Subsequent calls to <tt>mpp_read</tt>   return the field data arrays corresponding to
   the fieldtype.  The <tt>domain</tt>   type is an optional
   argument.  If <tt>domain</tt>   is omitted, the incoming field
   array should be dimensioned for the global domain, otherwise, the
   field data is assigned to the computational domain of a local array.
   <br>
<br> 
<i>Multi-fileset</i>   reads are not supported with <tt>mpp_read</tt>. </div>
<br>
<!-- END DESCRIPTION -->
<a name="OTHER%20MODULES%20USED"></a>
<hr>
<h4>OTHER MODULES USED</h4>
<!-- BEGIN OTHER MODULES USED -->
<div>
<pre>      mpp_data_mod<br>  mpp_datatype_mod<br> mpp_parameter_mod<br>   mpp_io_util_mod<br>   mpp_io_misc_mod<br>  mpp_io_write_mod<br>   mpp_io_read_mod<br>mpp_io_connect_mod</pre>
</div>
<!-- END OTHER MODULES USED -->
<!-- BEGIN PUBLIC INTERFACE -->
<a name="PUBLIC%20INTERFACE"></a>
<hr>
<h4>PUBLIC INTERFACE</h4>
<div>
<dl></dl>
</div>
<br>
<!-- END PUBLIC INTERFACE -->
<a name="PUBLIC%20DATA"></a>
<hr>
<h4>PUBLIC DATA</h4>
<!-- BEGIN PUBLIC DATA -->
<div>None.<br>
<br>
</div>
<!-- END PUBLIC DATA -->
<a name="PUBLIC%20ROUTINES"></a>
<hr>
<h4>PUBLIC ROUTINES</h4>
<!-- BEGIN PUBLIC ROUTINES -->
<ol type="a"></ol>
<!-- END PUBLIC ROUTINES -->
<a name="PUBLIC%20TYPES"></a>
<!-- BEGIN PUBLIC TYPES -->
<!-- END PUBLIC TYPES --><a name="NAMELIST"></a>
<!-- BEGIN NAMELIST -->
<!-- END NAMELIST --><a name="DIAGNOSTIC%20FIELDS"></a>
<!-- BEGIN DIAGNOSTIC FIELDS -->
<!-- END DIAGNOSTIC FIELDS --><a name="DATA%20SETS"></a>
<!-- BEGIN DATA SETS -->
<hr>
<h4>DATA SETS</h4>
<div>None.<br>
<br>
</div>
<!-- END DATA SETS -->
<a name="PUBLIC%20CODE"></a>
<!-- BEGIN PUBLIC CODE -->
<!-- END PUBLIC CODE --><a name="ERROR%20MESSAGES"></a>
<!-- BEGIN ERROR MESSAGES -->
<hr>
<h4>ERROR MESSAGES</h4>
<div>None.<br>
<br>
</div>
<!-- END ERROR MESSAGES -->
<hr>
<div align="right">
<font size="-1"><a href="#TOP">top</a></font>
</div>
</body>
</html>
